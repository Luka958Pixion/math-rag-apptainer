Waiting for the TGI server to become healthy...
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T12:59:32.622960Z  INFO text_generation_launcher: Args {
    model_id: "/model",
    revision: None,
    validation_workers: 2,
    sharded: None,
    num_shard: None,
    quantize: None,
    speculate: None,
    dtype: None,
    kv_cache_dtype: None,
    trust_remote_code: false,
    max_concurrent_requests: 128,
    max_best_of: 2,
    max_stop_sequences: 4,
    max_top_n_tokens: 5,
    max_input_tokens: None,
    max_input_length: None,
    max_total_tokens: None,
    waiting_served_ratio: 0.3,
    max_batch_prefill_tokens: None,
    max_batch_total_tokens: None,
    max_waiting_tokens: 20,
    max_batch_size: None,
    cuda_graphs: None,
    hostname: "0.0.0.0",
    port: 8000,
    shard_uds_path: "/tmp/text-generation-server",
    master_addr: "localhost",
    master_port: 29500,
    huggingface_hub_cache: None,
    weights_cache_override: None,
    disable_custom_kernels: false,
    cuda_memory_fraction: 1.0,
    rope_scaling: None,
    rope_factor: None,
    json_output: false,
    otlp_endpoint: None,
    otlp_service_name: "text-generation-inference.router",
    cors_allow_origin: [],
    api_key: None,
    watermark_gamma: None,
    watermark_delta: None,
    ngrok: false,
    ngrok_authtoken: None,
    ngrok_edge: None,
    tokenizer_config_path: None,
    disable_grammar_support: false,
    env: false,
    max_client_batch_size: 4,
    lora_adapters: None,
    usage_stats: On,
    payload_limit: 2000000,
    enable_prefill_logprobs: false,
}
2025-03-20T12:59:35.752334Z  INFO text_generation_launcher: Using attention flashinfer - Prefix caching true
2025-03-20T12:59:35.849322Z  INFO text_generation_launcher: Default `max_batch_prefill_tokens` to 131072
2025-03-20T12:59:35.849372Z  INFO text_generation_launcher: Using default cuda graphs [1, 2, 4, 8, 16, 32]
2025-03-20T12:59:35.849505Z  INFO download: text_generation_launcher: Starting check and download process for /model
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T12:59:40.679062Z  INFO text_generation_launcher: Files are already present on the host. Skipping download.
2025-03-20T12:59:41.560445Z  INFO download: text_generation_launcher: Successfully downloaded weights for /model
2025-03-20T12:59:41.560658Z  INFO shard-manager: text_generation_launcher: Starting shard rank=0
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T12:59:45.168281Z  INFO text_generation_launcher: Using prefix caching = True
2025-03-20T12:59:45.169066Z  INFO text_generation_launcher: Using Attention = flashinfer
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T12:59:51.572737Z  INFO shard-manager: text_generation_launcher: Waiting for shard to be ready... rank=0
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T12:59:55.297949Z  INFO text_generation_launcher: Using prefill chunking = True
2025-03-20T12:59:55.384168Z  INFO text_generation_launcher: Server started at unix:///tmp/text-generation-server-0
2025-03-20T12:59:55.476110Z  INFO shard-manager: text_generation_launcher: Shard ready in 13.910539841s rank=0
2025-03-20T12:59:55.569130Z  INFO text_generation_launcher: Starting Webserver
2025-03-20T12:59:55.739091Z  INFO text_generation_router_v3: backends/v3/src/lib.rs:125: Warming up model
2025-03-20T12:59:56.154073Z  INFO text_generation_launcher: Using optimized Triton indexing kernels.
Health check returned status code 000. Retrying in 5 seconds...
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T13:00:05.232082Z  INFO text_generation_launcher: KV-cache blocks: 668320, size: 1
2025-03-20T13:00:05.293564Z  INFO text_generation_launcher: Cuda Graphs are enabled for sizes [32, 16, 8, 4, 2, 1]
2025-03-20T13:00:06.514000Z  INFO text_generation_router_v3: backends/v3/src/lib.rs:137: Setting max batch total tokens to 668320
2025-03-20T13:00:06.514017Z  WARN text_generation_router_v3::backend: backends/v3/src/backend.rs:39: Model supports prefill chunking. `waiting_served_ratio` and `max_waiting_tokens` will be ignored.
2025-03-20T13:00:06.514026Z  INFO text_generation_router_v3: backends/v3/src/lib.rs:166: Using backend V3
2025-03-20T13:00:06.514029Z  INFO text_generation_router: backends/v3/src/main.rs:162: Maximum input tokens defaulted to 131071
2025-03-20T13:00:06.514032Z  INFO text_generation_router: backends/v3/src/main.rs:168: Maximum total tokens defaulted to 131072
2025-03-20T13:00:06.516847Z  WARN text_generation_router::server: router/src/server.rs:1648: Tokenizer_config None - Some("/model/tokenizer_config.json")
Health check returned status code 000. Retrying in 5 seconds...
2025-03-20T13:00:09.945763Z  INFO text_generation_router::server: router/src/server.rs:1716: Using config Some(Llama)
2025-03-20T13:00:09.945838Z  WARN text_generation_router::server: router/src/server.rs:1776: no pipeline tag found for model /model
2025-03-20T13:00:09.990215Z  INFO text_generation_router::server: router/src/server.rs:2266: Connected
TGI server is healthy.
2025-03-20T13:00:13.217411Z  INFO text_generation_router_v3::radix: backends/v3/src/radix.rs:108: Prefix 0 - Suffix 55
2025-03-20T13:00:13.676056Z  INFO compat_generate{default_return_full_text=true compute_type=Extension(ComputeType("1-nvida-a100-sxm4-80gb"))}:generate{parameters=GenerateParameters { best_of: None, temperature: None, repetition_penalty: None, frequency_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(50), return_full_text: Some(false), stop: [], truncate: None, watermark: false, details: true, decoder_input_details: false, seed: None, top_n_tokens: None, grammar: None, adapter_id: None } total_time="458.944342ms" validation_time="289.489µs" queue_time="62.12µs" inference_time="458.593014ms" time_per_token="9.17186ms" seed="None"}: text_generation_router::server: router/src/server.rs:424: Success
Generated Text:  I’ll tell you a joke.
I’m a little bit of a jokester. I like to tell jokes. I like to tell jokes about myself. I like to tell jokes about other people. I like to tell jokes about the world. I